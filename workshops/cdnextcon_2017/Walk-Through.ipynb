{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here is walk-through to help getting started with tensorflow\n",
    "\n",
    "1) Simple Linear Regression with low-level TensorFlow  \n",
    "2) Simple Linear Regression with a canned estimator  \n",
    "3) Playing with real data: linear regressor and DNN  \n",
    "4) Building a custom estimator to classify handwritten digits (MNIST)\n",
    "\n",
    "### What's next? https://goo.gl/hZaLPA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# tensorflow\n",
    "import tensorflow as tf\n",
    "print('Expected TensorFlow version is v1.3.0 or higher')\n",
    "print('Your TensorFlow version:', tf.__version__)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# visualization\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams['figure.figsize'] = [12,8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Simple Linear Regression with low-level TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data\n",
    "\n",
    "This function creates a noisy dataset that's roughly linear, according to the equation y = mx + b + noise.\n",
    "\n",
    "Notice that the expected value for m is 0.1 and for b is 0.3. This is the values we expect the model to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_noisy_data(m=0.1, b=0.3, n=100):\n",
    "    x = np.random.randn(n)\n",
    "    noise = np.random.normal(scale=0.01, size=len(x))\n",
    "    y = m * x + b + noise\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_train, y_train = make_noisy_data()\n",
    "x_test, y_test = make_noisy_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.plot(x_train, y_train, 'b.')\n",
    "plt.plot(x_test, y_test, 'g.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input and output\n",
    "x = tf.placeholder(shape=[None], dtype=tf.float32, name='x')\n",
    "y_label = tf.placeholder(shape=[None], dtype=tf.float32, name='y_label')\n",
    "\n",
    "# variables\n",
    "W = tf.Variable(tf.random_normal([1], name=\"W\")) # weight\n",
    "b = tf.Variable(tf.random_normal([1], name=\"b\")) # bias\n",
    "\n",
    "# actual model\n",
    "y = W * x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Loss and Optimizer\n",
    "\n",
    "Define a loss function (here, squared error) and an optimizer (here, gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(y - y_label))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1)\n",
    "train = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop and generating predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "  sess.run(init) # initialize variables\n",
    "  for i in range(100): # train for 100 steps\n",
    "    sess.run(train, feed_dict={x: x_train, y_label:y_train})\n",
    "\n",
    "  fake_data = np.linspace(-3, 3, 101)\n",
    "  # using the trained model to predict values for the training data\n",
    "  pred_train = sess.run(y, feed_dict={x: fake_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot training\n",
    "plt.scatter(x_train, y_train)\n",
    "plt.plot(fake_data, pred_train, 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is the final weight and bias?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('W:', final_W, 'expected: 0.1')\n",
    "print('b:', final_b, 'expected: 0.3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Simple Linear Regression with a canned estimator  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x_dict = {'x': x_train}\n",
    "train_input = tf.estimator.inputs.numpy_input_fn(x_dict, y_train,\n",
    "                                                 shuffle=True,\n",
    "                                                 num_epochs=None) # repeat forever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe input feature usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [tf.feature_column.numeric_column('x')] # because x is a real number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearRegressor(features)\n",
    "estimator.train(train_input, steps = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make some predictions with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_dict = {'x': np.linspace(-5, 5, 11)}\n",
    "data_source = tf.estimator.inputs.numpy_input_fn(x_test_dict, shuffle=False)\n",
    "\n",
    "for y in estimator.predict(data_source):\n",
    "    print(y['predictions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Playing with real data: linear regressor and DNN  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "census_train_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
    "census_train_path = tf.contrib.keras.utils.get_file('census.train', census_train_url)\n",
    "\n",
    "census_test_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test'\n",
    "census_test_path = tf.contrib.keras.utils.get_file('census.test', census_test_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_names = [\n",
    "  'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "  'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "  'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n",
    "  'income'\n",
    "]\n",
    "\n",
    "census_train = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "census_test = pd.read_csv(census_train_path, index_col=False, names=column_names) \n",
    "\n",
    "census_train_label = census_train.pop('income') == \" >50K\" \n",
    "census_test_label = census_test.pop('income') == \" >50K\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "census_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_input = tf.estimator.inputs.pandas_input_fn(\n",
    "    census_train, \n",
    "    census_train_label,\n",
    "    shuffle=True, \n",
    "    num_epochs=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = train_input()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    tf.feature_column.numeric_column('education-num'),\n",
    "    tf.feature_column.numeric_column('hours-per-week'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('age'), list(range(0,100,10))),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('sex',['male','female']),\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearClassifier(features, n_classes=2, model_dir='census/linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.train(train_input, steps=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input = tf.estimator.inputs.pandas_input_fn(\n",
    "    census_test, \n",
    "    census_test_label, \n",
    "    shuffle=True, \n",
    "    num_epochs=1)\n",
    "\n",
    "estimator.evaluate(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the results with [TensorBoard](http://0.0.0.0:6006)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "tensorboard --logdir census"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DNN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update input pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dimension 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-134-b2e4e9777f0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategorical_column_with_vocabulary_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'sex'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'male'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'female'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     tf.feature_column.embedding_column(\n\u001b[0;32m----> 7\u001b[0;31m         tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000), 0)\n\u001b[0m\u001b[1;32m      8\u001b[0m ]\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36membedding_column\u001b[0;34m(categorical_column, dimension, combiner, initializer, ckpt_to_load_from, tensor_name_in_ckpt, max_norm, trainable)\u001b[0m\n\u001b[1;32m    489\u001b[0m   \"\"\"\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid dimension {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mckpt_to_load_from\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtensor_name_in_ckpt\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     raise ValueError('Must specify both `ckpt_to_load_from` and '\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid dimension 0."
     ]
    }
   ],
   "source": [
    "features = [\n",
    "    tf.feature_column.numeric_column('education-num'),\n",
    "    tf.feature_column.numeric_column('hours-per-week'),\n",
    "    tf.feature_column.bucketized_column(tf.feature_column.numeric_column('age'), list(range(0,100,10))),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list('sex',['male','female']),\n",
    "    tf.feature_column.embedding_column(\n",
    "        tf.feature_column.categorical_column_with_hash_bucket('native-country', 1000), 0)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'DNNClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-1aab733e7875>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDNNClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'census/linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'DNNClassifier'"
     ]
    }
   ],
   "source": [
    "estimator = tf.estimator.DNNClassifier(features, n_classes=2, model_dir='census/linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression - Custom Input Pipeline using Datasets API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imports85_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data'\n",
    "imports85_path = tf.contrib.keras.utils.get_file('imports85.data', imports85_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def imports_85():\n",
    "    def remove_question_marks(line):\n",
    "        return tf.py_func(lambda x: \"?\" not in x, [line], tf.bool)\n",
    "    \n",
    "    dataset = (\n",
    "        tf.contrib.data.TextLineDataset(imports85_path)\n",
    "            .filter(remove_question_marks)\n",
    "            .map(csv_decoder)\n",
    "            .shuffle(buffer_size=100)\n",
    "            .batch(32)\n",
    "            .repeat())\n",
    "    \n",
    "    columns = dataset.make_one_shot_iterator().get_next()\n",
    "    \n",
    "    price = columns.pop('price')\n",
    "    \n",
    "    return columns, price/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_defaults = collections.OrderedDict([\n",
    "    ('symboling', [0]),\n",
    "    ('normalized-losses', [0.]),\n",
    "    ('make', [\"\"]),\n",
    "    ('fuel-type', [\"\"]),\n",
    "    ('aspiration', [\"\"]),\n",
    "    ('num-of-doors', [\"\"]),\n",
    "    ('body-style', [\"\"]),\n",
    "    ('drive-wheels', [\"\"]),\n",
    "    ('engine-location', [\"\"]),\n",
    "    ('wheel-base', [0.]),\n",
    "    ('length', [0.]),\n",
    "    ('width', [0.]),\n",
    "    ('height', [0.]),\n",
    "    ('curb-weight', [0.]),\n",
    "    ('engine-type', [\"\"]),\n",
    "    ('num-of-cylynders', [\"\"]),\n",
    "    ('engine-size', [0.]),\n",
    "    ('fuel-system', [\"\"]),\n",
    "    ('bore', [0.]),\n",
    "    ('stroke', [0.]),\n",
    "    ('compression-ratio', [0.]), \n",
    "    ('horsepower', [0.]),\n",
    "    ('peak-rpm', [0.]),\n",
    "    ('city-mpg', [0.]),\n",
    "    ('highway-mpg', [0.]),\n",
    "    ('price', [0.])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_decoder(line):\n",
    "  parsed = tf.decode_csv(line, csv_defaults.values())\n",
    "\n",
    "  return dict(zip(csv_defaults.keys(), parsed))\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "training_batch = imports_85()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    features, price = sess.run(training_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['highway-mpg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(features['body-style'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "    tf.feature_column.numeric_column('curb-weight'),\n",
    "    tf.feature_column.numeric_column('highway-mpg'),\n",
    "    tf.feature_column.categorical_column_with_vocabulary_list(\n",
    "        'body-style', ['sedan', 'hatchback', 'wagon','hardtop','convertible']),\n",
    "    tf.feature_column.categorical_column_with_hash_bucket('make', 256)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = tf.estimator.LinearRegressor(features)\n",
    "estimator.train(imports_85, steps = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.8236e+08**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Building a custom estimator to classify handwritten digits (MNIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
