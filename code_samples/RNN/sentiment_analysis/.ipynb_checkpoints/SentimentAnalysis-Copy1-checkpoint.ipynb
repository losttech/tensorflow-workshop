{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mDT8S9C9CYtr"
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Feeding function for enqueue data\n",
    "# \n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions as ff\n",
    "\n",
    "# Rnn common functions\n",
    "from tensorflow.contrib.learn.python.learn.estimators import rnn_common\n",
    "\n",
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Input function\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "\n",
    "# Helpers for data processing\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UrAyWt23AtCM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "there are 62702 words in the train and test files\n"
     ]
    }
   ],
   "source": [
    "# data from: http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "TRAIN_INPUT = 'data/train.csv'\n",
    "TEST_INPUT = 'data/test.csv'\n",
    "\n",
    "# data manually generated\n",
    "MY_TEST_INPUT = 'data/mytest.csv'\n",
    "\n",
    "# Parameters for training\n",
    "STEPS = 1000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for data processing\n",
    "SEQUENCE_LENGTH_KEY = 'sequence_length'\n",
    "REVIEW_KEY = 'review'\n",
    "CLASSIFICATION_KEY = 'is_positive'\n",
    "\n",
    "# Vocabulary size\n",
    "VOCAB_FILE = 'data/vocab.txt'\n",
    "VOCAB = [line[:len(line)-1] for line in open(VOCAB_FILE)]\n",
    "VOCAB_SIZE = len(VOCAB)\n",
    "\n",
    "print('there are %s words in the train and test files' % VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0dlZ9C27M-bS"
   },
   "outputs": [],
   "source": [
    "# This function creates a sparse tensor in the following way, given:\n",
    "# indices = [[0, 0], [1, 1], [2, 2]]\n",
    "# values = [1, 2, 3]\n",
    "# dense_shape = [3, 4]\n",
    "#\n",
    "# The output will be a sparse tensor that represents this dense tensor:\n",
    "# [ \n",
    "#   [1, 0, 0, 0]\n",
    "#   [0, 2, 0, 0]\n",
    "#   [0, 0, 3, 0]\n",
    "# ]\n",
    "#\n",
    "# We're using this to generate a Sparse tensor that can be easily\n",
    "# formated in a one hot representation.\n",
    "# More at: https://www.tensorflow.org/api_docs/python/tf/SparseTensor\n",
    "def _sparse_string_to_index(sp, mapping):\n",
    "    return tf.SparseTensor(indices=sp.indices,\n",
    "                           values=tf.contrib.lookup.string_to_index(sp.values,\n",
    "                                                                    mapping),\n",
    "                           dense_shape=sp.dense_shape)\n",
    "\n",
    "def array_to_onehot(array, num_dim=2):\n",
    "    array = np.asarray(array, dtype=np.int32)\n",
    "    onehot = np.zeros([array.shape[0], num_dim])\n",
    "    for i in range(array.shape[0]):\n",
    "        onehot[i][array[i]] = 1\n",
    "    return onehot\n",
    "\n",
    "# Returns the column values from a CSV file as a list\n",
    "def _get_csv_column(csv_file, column_name):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        return df[column_name].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input function used for training and testing                                                \n",
    "def get_input_fn(csv_file, batch_size, epochs=1):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        def input_fn():\n",
    "            # Using queue with multiple threads to make it scalable\n",
    "            pandas_queue = ff._enqueue_data(df,\n",
    "                                            capacity=1024,\n",
    "                                            shuffle=True,\n",
    "                                            min_after_dequeue=256,\n",
    "                                            num_threads=4,\n",
    "                                            enqueue_size=16,\n",
    "                                            num_epochs=epochs)\n",
    "\n",
    "            _, review, classification, seq_len = pandas_queue.dequeue_up_to(batch_size)\n",
    "            \n",
    "            # Split sentences into words\n",
    "            split_review = tf.string_split(review, delimiter=' ')\n",
    "            # Creating a tf constant to hold the word -> index\n",
    "            # this is need to create the sparse tensor and after the one hot encode\n",
    "            mapping = tf.constant(VOCAB, name=\"mapping\")\n",
    "            # Words represented in a sparse tensor\n",
    "            integerized_review = _sparse_string_to_index(split_review, mapping)\n",
    "\n",
    "            # Converting numbers to int 32\n",
    "            classification = tf.cast(classification, tf.int32)\n",
    "            seq_len = tf.cast(seq_len, tf.int32)\n",
    "            \n",
    "            # Generates batcheds\n",
    "            batched = tf.train.shuffle_batch({REVIEW_KEY: integerized_review,\n",
    "                                              SEQUENCE_LENGTH_KEY: seq_len,\n",
    "                                              CLASSIFICATION_KEY: classification},\n",
    "                                             batch_size,\n",
    "                                             min_after_dequeue=100,\n",
    "                                             num_threads=4,\n",
    "                                             capacity=1000,\n",
    "                                             enqueue_many=True,\n",
    "                                             allow_smaller_final_batch=True)\n",
    "            \n",
    "            label = batched.pop(CLASSIFICATION_KEY)\n",
    "            label_onehot = tf.one_hot(label, 2)\n",
    "            return batched, label_onehot\n",
    "    return input_fn\n",
    "\n",
    "# Creating my own input function for a custom CSV file\n",
    "# it's simpler than the input_fn above but just used for small tests\n",
    "def get_my_input_fn():\n",
    "    def _input_fn():\n",
    "        with open(MY_TEST_INPUT, 'r') as f:\n",
    "            df = pd.read_csv(f)\n",
    "\n",
    "            review = df.review.tolist()\n",
    "\n",
    "            # Split sentences into words\n",
    "            split_review = tf.string_split(review, delimiter=' ')\n",
    "            # Creating a tf constant to hold the word -> index\n",
    "            # this is need to create the sparse tensor and after the one hot encode\n",
    "            mapping = tf.constant(VOCAB, name=\"mapping\")\n",
    "            # Words represented in a sparse tensor\n",
    "            integerized_review = _sparse_string_to_index(split_review, mapping)\n",
    "            \n",
    "            x = {REVIEW_KEY: integerized_review, SEQUENCE_LENGTH_KEY: df.sequence_length.tolist()}\n",
    "\n",
    "            y = df.is_positive.tolist()\n",
    "\n",
    "            return x, y\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m5UJyvW5P0Sy"
   },
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE, None)\n",
    "test_input_fn = get_input_fn(TEST_INPUT, BATCH_SIZE)\n",
    "my_test_input_fn = get_my_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lE4c3ELMQjHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-82d13ba8809b>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "({'sequence_length': <tf.Tensor 'shuffle_batch:2' shape=(?,) dtype=int32>, 'review': <tensorflow.python.framework.sparse_tensor.SparseTensor object at 0x7fa73d65b940>}, <tf.Tensor 'one_hot:0' shape=(?, 2) dtype=float32>)\n",
      "({'sequence_length': array([80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
      "       80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,\n",
      "       80, 68, 80, 80, 80, 80, 80, 80, 80, 80, 58, 80, 80, 65, 80, 80, 80,\n",
      "       80, 49, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80, 80], dtype=int32), 'review': SparseTensorValue(indices=array([[ 0,  0],\n",
      "       [ 0,  1],\n",
      "       [ 0,  2],\n",
      "       ..., \n",
      "       [63, 77],\n",
      "       [63, 78],\n",
      "       [63, 79]]), values=array([    0,     8,    11, ...,  1057,  2177, 16111]), dense_shape=array([64, 80]))}, array([[ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.],\n",
      "       [ 1.,  0.],\n",
      "       [ 1.,  0.],\n",
      "       [ 0.,  1.],\n",
      "       [ 0.,  1.]], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "# Testing the input function\n",
    "with tf.Graph().as_default():\n",
    "    train_input = train_input_fn()\n",
    "    with tf.train.MonitoredSession() as sess:\n",
    "        print (train_input)\n",
    "        print (sess.run(train_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VxXAUrYN7TvR"
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01,\n",
    "                 embed_dim=64):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        review = features[REVIEW_KEY]\n",
    "        sequence_length = tf.cast(features[SEQUENCE_LENGTH_KEY], tf.int32)\n",
    "        labels_onehot = labels\n",
    "        \n",
    "        # Creating dense representation for the sentences\n",
    "        # and then converting it to embeding representation\n",
    "        dense_review = tf.sparse_tensor_to_dense(review, default_value=VOCAB_SIZE)\n",
    "        embed_review = tf.contrib.layers.embed_sequence(dense_review,\n",
    "                                                        vocab_size=(VOCAB_SIZE + 1),\n",
    "                                                        embed_dim=embed_dim)\n",
    "        \n",
    "        \n",
    "        # Each RNN layer will consist of a LSTM cell\n",
    "        rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=embed_review,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "        predictions_softmax = tf.nn.softmax(predictions)\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = None\n",
    "        train_op = None\n",
    "        \n",
    "        preds_op = {\n",
    "            'prediction': predictions_softmax,\n",
    "            'label': labels_onehot\n",
    "        }\n",
    "        \n",
    "        eval_op = {\n",
    "            \"accuracy\": tf.metrics.accuracy(\n",
    "                     tf.argmax(input=predictions_softmax, axis=1),\n",
    "                     tf.argmax(input=labels_onehot, axis=1))\n",
    "        }\n",
    "        \n",
    "        if mode != tf.contrib.learn.ModeKeys.INFER:    \n",
    "            loss = tf.losses.softmax_cross_entropy(labels_onehot, predictions)\n",
    "    \n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return tf.contrib.learn.ModelFnOps(mode,\n",
    "                                           predictions=predictions_softmax,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op,\n",
    "                                           eval_metric_ops=eval_op)\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gUHR3Mzc7Tvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_tf_random_seed': None, '_is_chief': True, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fa707b54710>, '_save_summary_steps': 100, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_task_id': 0, '_task_type': None, '_num_worker_replicas': 0, '_num_ps_replicas': 0, '_environment': 'local', '_evaluation_master': '', '_model_dir': None, '_save_checkpoints_steps': None, '_keep_checkpoint_max': 5}\n"
     ]
    }
   ],
   "source": [
    "model_fn = get_model_fn(rnn_cell_sizes=[64], # size of the hidden layers\n",
    "                        label_dimension=2, # since are just 2 classes\n",
    "                        dnn_layer_sizes=[64], # size of units in the dense layers on top of the RNN\n",
    "                        optimizer='Adam',\n",
    "                        learning_rate=0.001)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model_fn, model_dir='output/dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DUZEKQrdGgZE"
   },
   "outputs": [],
   "source": [
    "# create experiment\n",
    "def generate_experiment_fn():\n",
    "  \n",
    "  \"\"\"\n",
    "  Create an experiment function given hyperparameters.\n",
    "  Returns:\n",
    "    A function (output_dir) -> Experiment where output_dir is a string\n",
    "    representing the location of summaries, checkpoints, and exports.\n",
    "    this function is used by learn_runner to create an Experiment which\n",
    "    executes model code provided in the form of an Estimator and\n",
    "    input functions.\n",
    "    All listed arguments in the outer function are used to create an\n",
    "    Estimator, and input functions (training, evaluation, serving).\n",
    "    Unlisted args are passed through to Experiment.\n",
    "  \"\"\"\n",
    "\n",
    "  def _experiment_fn(output_dir):\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=test_input_fn,\n",
    "        train_steps=STEPS\n",
    "    )\n",
    "  return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "run() missing 1 required positional argument: 'output_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-e204c881a73d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# run experiment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlearn_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerate_experiment_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: run() missing 1 required positional argument: 'output_dir'"
     ]
    }
   ],
   "source": [
    "# run experiment \n",
    "learn_runner.run(generate_experiment_fn(), '/tmp/output_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-290-82d13ba8809b>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpi20xxiea/model.ckpt-10000\n"
     ]
    }
   ],
   "source": [
    "preds = list(estimator.predict(input_fn=test_input_fn, as_iterable=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"print(preds[0])\\nfor i in range(5):\\n    p = preds[i]['prediction']\\n    print('bad review:', p[0], 'good review:', p[1])\\n    print('reality:', p['label'])\\n    print('-' * 10)\\n\""
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''print(preds[0])\n",
    "for i in range(5):\n",
    "    p = preds[i]['prediction']\n",
    "    print('bad review:', p[0], 'good review:', p[1])\n",
    "    print('reality:', p['label'])\n",
    "    print('-' * 10)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My own predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-290-82d13ba8809b>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpi20xxiea/model.ckpt-10000\n",
      "\n",
      "sentence: this is a great movie\n",
      "bad review: 0.00432568 good review: 0.995674\n",
      "----------\n",
      "sentence: this is a good movie but isnt the best\n",
      "bad review: 0.101033 good review: 0.898967\n",
      "----------\n",
      "sentence: this is a ok movie\n",
      "bad review: 0.398496 good review: 0.601504\n",
      "----------\n",
      "sentence: this movie sucks\n",
      "bad review: 0.999955 good review: 4.5195e-05\n",
      "----------\n",
      "sentence: this movie sucks but isnt the worst\n",
      "bad review: 0.998109 good review: 0.00189128\n",
      "----------\n",
      "sentence: its not that bad\n",
      "bad review: 0.991135 good review: 0.0088646\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "preds = estimator.predict(input_fn=my_test_input_fn, as_iterable=True)\n",
    "\n",
    "sentences = _get_csv_column(MY_TEST_INPUT, 'review')\n",
    "\n",
    "print()\n",
    "for p, s in zip(preds, sentences):\n",
    "    print('sentence:', s)\n",
    "    print('bad review:', p[0], 'good review:', p[1])\n",
    "    print('-' * 10)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//experimental/users/jamieas/transform_colab:notebook",
    "kind": "private"
   },
   "name": "Copy of CustomEstimator.ipynb",
   "provenance": [
    {
     "file_id": "0BwN-JPfIIHwgdFkwUTVIWTQwU00",
     "timestamp": 1496845355496
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
