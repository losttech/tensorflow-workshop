{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Catigorical Columns Don't just One-Hot, They Count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have 3 examples, each containing 5 strings.\n",
    "\n",
    ">Note: empty strings are ignored, and can be used as padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "strings = np.array([['a','a','','b','c'],['a','c','zz','',''],['b','qq','qq','b','']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define a categorical column to represent it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab = np.array(['a','b','c','<UNK>'],dtype=object)\n",
    "sparse = tf.feature_column.categorical_column_with_vocabulary_list('strings',vocab, default_value=len(vocab)-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use `indicator_column` to define a dense representation, and `input_layer` to build the conversion operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = tf.feature_column.input_layer({'strings':strings}, [tf.feature_column.indicator_column(sparse)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.train.MonitoredSession() as sess:\n",
    "    input_value = sess.run(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(input_value, columns=vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Columns `reduce` Over Entries, Using the `combiner` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "layer = tf.feature_column.input_layer(\n",
    "    {'strings':strings},\n",
    "    [tf.feature_column.embedding_column(sparse,10, combiner='mean')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "    sess.run(init)\n",
    "    input_value = sess.run(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is `3`, `10d` embeddings, because it takes the mean over the strings in each example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To Skip the `reduce`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use `sequence_input_from_feature_columns`\n",
    "\n",
    "> Note: this is only compatible with `contrib` feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sparse = tf.contrib.layers.sparse_column_with_keys('strings', vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = tf.contrib.layers.sequence_input_from_feature_columns(\n",
    "    {'strings':tf.constant(strings)},\n",
    "    [tf.contrib.layers.embedding_column(sparse ,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this, it gives a 10d embedding for each of the 5 strings in each of the 3 examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "    sess.run(init)\n",
    "    input_value = sess.run(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Or some careful reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "shape = tf.shape(strings) \n",
    "embedding_dim = 10\n",
    "\n",
    "layer = tf.feature_column.input_layer(\n",
    "    {'strings':tf.reshape(strings,[tf.reduce_prod(shape)])},\n",
    "    [tf.feature_column.embedding_column(sparse,embedding_dim, combiner='mean')])\n",
    "\n",
    "layer = tf.reshape(layer,tf.concat([shape, [embedding_dim]],0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "    sess.run(init)\n",
    "    input_value = sess.run(layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_value.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Bag of Words Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the IMDB dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_WORDS=1000 # only use top 1000 words\n",
    "MAX_LEN=250    # truncate after 250 words\n",
    "INDEX_FROM=3   # word index offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,test = keras.datasets.imdb.load_data(maxlen=MAX_LEN, num_words=NUM_WORDS, index_from=INDEX_FROM)\n",
    "train_x,train_y = train\n",
    "test_x,test_y = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Look at an example review\n",
    "\n",
    "(Punctuation and capitalization are stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = keras.datasets.imdb.get_word_index()\n",
    "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
    "word_to_id[\"<PAD>\"] = 0\n",
    "word_to_id[\"<START>\"] = 1\n",
    "word_to_id[\"<UNK>\"] = 2\n",
    "\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "print(' '.join(id_to_word[id] for id in train_x[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(x_in, y_in, shuffle=True, epochs=1):\n",
    "    def input_fn():\n",
    "        ys = tf.contrib.data.Dataset.from_tensor_slices(y_in)\n",
    "        \n",
    "        # Convert x_in to a sparse tensor\n",
    "        nested_sparse = [\n",
    "            (np.array([[n]*len(x),range(len(x))]).T,x)\n",
    "            for n,x in enumerate(x_in)\n",
    "        ]\n",
    "        \n",
    "        indices = np.concatenate([idx for idx,value in nested_sparse], axis = 0)\n",
    "        values = np.concatenate([value for idx,value in nested_sparse], axis = 0)\n",
    "        \n",
    "        max_len = max(len(ex) for ex in x_in)\n",
    "        xs = tf.SparseTensor(indices = indices, values = values, dense_shape=[25000, max_len])\n",
    "        \n",
    "        xs = tf.contrib.data.Dataset.from_sparse_tensor_slices(xs)\n",
    "        \n",
    "        xs = xs.map(lambda *x: tf.sparse_tensor_to_dense(tf.SparseTensor(*x)))\n",
    "        \n",
    "        ds = tf.contrib.data.Dataset.zip([xs,ys]).repeat(epochs)\n",
    "        \n",
    "        if shuffle:\n",
    "            ds = ds.shuffle(10000)\n",
    "            \n",
    "        ds = ds.batch(32)\n",
    "\n",
    "        x,y = ds.make_one_shot_iterator().get_next()\n",
    "\n",
    "        return {'word_ids':x},y\n",
    "        \n",
    "    return input_fn\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_fn = get_input_fn(x_in = np.array([[1,1,1],[2,2],[3,3,3],[4,4,4,4],[5],[6,6],[7,7,7,7,7]]), \n",
    "                     y_in = np.array([1,2,3,4,5,6,7]))\n",
    "\n",
    "x,y = in_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "    sess.run(init)\n",
    "    x,y = sess.run([x,y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the zero padding, and that `x` and `y` have the same shuffle applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x['word_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Estimator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_ids = tf.feature_column.categorical_column_with_identity('word_ids', NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bow_estimator = tf.contrib.learn.LinearClassifier(feature_columns=[word_ids], model_dir='tensorboard/BOW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(25):\n",
    "    bow_estimator.fit(input_fn=get_input_fn(train_x,train_y,epochs=10))\n",
    "    bow_estimator.evaluate(input_fn=get_input_fn(test_x, test_y,epochs=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## DNN Bag of words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DNN_bow_estimator = tf.contrib.learn.DNNClassifier(\n",
    "    [256, 256],  model_dir='tensorboard/DNN_BOW',\n",
    "    feature_columns=[tf.feature_column.embedding_column(word_ids, 30, combiner='mean')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for n in range(25):\n",
    "    DNN_bow_estimator.fit(input_fn=get_input_fn(train_x,train_y,epochs=10))\n",
    "    DNN_bow_estimator.evaluate(input_fn=get_input_fn(test_x,test_y,epochs=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
