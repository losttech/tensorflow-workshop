{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mDT8S9C9CYtr"
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Feeding function for enqueue data\n",
    "# \n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions as ff\n",
    "\n",
    "# Rnn common functions\n",
    "from tensorflow.contrib.learn.python.learn.estimators import rnn_common\n",
    "\n",
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Input function\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "\n",
    "# Helpers for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plot images with pyplot\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UrAyWt23AtCM"
   },
   "outputs": [],
   "source": [
    "# Data files\n",
    "TRAIN_INPUT = 'data/train.csv'\n",
    "TEST_INPUT = 'data/test.csv'\n",
    "MY_TEST_INPUT = 'data/mytest.csv'\n",
    "\n",
    "# Parameters for training\n",
    "STEPS = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for data processing\n",
    "CHARACTERS = [chr(i) for i in range(256)]\n",
    "\n",
    "SEQUENCE_LENGTH_KEY = 'sequence_length'\n",
    "COLOR_NAME_KEY = 'color_name'\n",
    "RGB_KEY = 'rgb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0dlZ9C27M-bS"
   },
   "outputs": [],
   "source": [
    "# This function creates a sparse tensor in the following way, given:\n",
    "# indices = [[0, 0], [1, 1], [2, 2]]\n",
    "# values = [1, 2, 3]\n",
    "# dense_shape = [3, 4]\n",
    "#\n",
    "# The output will be a sparse tensor that represents this dense tensor:\n",
    "# [ \n",
    "#   [1, 0, 0, 0]\n",
    "#   [0, 2, 0, 0]\n",
    "#   [0, 0, 3, 0]\n",
    "# ]\n",
    "#\n",
    "# We're using this to generate a Sparse tensor that can be easily\n",
    "# formated in a one hot representation.\n",
    "# More at: https://www.tensorflow.org/api_docs/python/tf/SparseTensor\n",
    "def _sparse_string_to_index(sp, mapping):\n",
    "    return tf.SparseTensor(indices=sp.indices,\n",
    "                           values=tf.contrib.lookup.string_to_index(sp.values,\n",
    "                                                                    mapping),\n",
    "                           dense_shape=sp.dense_shape)\n",
    "\n",
    "# Returns the column values from a CSV file as a list\n",
    "def _get_csv_column(csv_file, column_name):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        return df[column_name].tolist()\n",
    "\n",
    "# Plot a color image\n",
    "def _plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input function used for training and testing                                                \n",
    "def get_input_fn(csv_file, batch_size, epochs=1):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        # Sequence length is used by the Dynamic RNN\n",
    "        # to dynamically unroll the graph :D!\n",
    "        df['sequence_length'] = df.name.str.len().astype(np.int32)\n",
    "\n",
    "        def input_fn():\n",
    "            # Using queue with multiple threads to make it scalable\n",
    "            pandas_queue = ff._enqueue_data(df,\n",
    "                                            capacity=1024,\n",
    "                                            shuffle=True,\n",
    "                                            min_after_dequeue=256,\n",
    "                                            num_threads=4,\n",
    "                                            enqueue_size=16,\n",
    "                                            num_epochs=epochs)\n",
    "\n",
    "            _, color_name, r, g, b, seq_len = pandas_queue.dequeue_up_to(batch_size)\n",
    "\n",
    "            # Split strings into chars\n",
    "            split_color_name = tf.string_split(color_name, delimiter='')\n",
    "            # Creating a tf constant to hold the map char -> index\n",
    "            # this is need to create the sparse tensor and after the one hot encode\n",
    "            mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "            # Names represented in a sparse tensor\n",
    "            integerized_color_name = _sparse_string_to_index(split_color_name, mapping)\n",
    "\n",
    "            # Tensor of normalized RGB values\n",
    "            rgb = tf.to_float(tf.stack([r, g, b], axis=1)) / 255.0\n",
    "\n",
    "            # Generates batcheds\n",
    "            batched = tf.train.shuffle_batch({COLOR_NAME_KEY: integerized_color_name,\n",
    "                                              SEQUENCE_LENGTH_KEY: seq_len,\n",
    "                                              RGB_KEY: rgb},\n",
    "                                             batch_size,\n",
    "                                             min_after_dequeue=100,\n",
    "                                             num_threads=4,\n",
    "                                             capacity=1000,\n",
    "                                             enqueue_many=True,\n",
    "                                             allow_smaller_final_batch=True)\n",
    "            label = batched.pop(RGB_KEY)\n",
    "            return batched, label\n",
    "    return input_fn\n",
    "\n",
    "# Creating my own input function for a custom CSV file\n",
    "# it's simpler than the input_fn above but just used for small tests\n",
    "def get_my_input_fn():\n",
    "    def _input_fn():\n",
    "        with open(MY_TEST_INPUT, 'r') as f:\n",
    "            df = pd.read_csv(f)\n",
    "            df['sequence_length'] = df.name.str.len().astype(np.int32)\n",
    "\n",
    "            color_name = df.name.tolist()\n",
    "    \n",
    "            split_color_name = tf.string_split(color_name, delimiter='')\n",
    "            mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "            integerized_color_name = _sparse_string_to_index(split_color_name, mapping)\n",
    "\n",
    "            x = {COLOR_NAME_KEY: integerized_color_name, SEQUENCE_LENGTH_KEY: df.sequence_length.tolist()}\n",
    "\n",
    "            y = np.asarray([[0, 0, 0]], dtype=np.float32)\n",
    "\n",
    "            return x, y\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m5UJyvW5P0Sy"
   },
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE, None)\n",
    "test_input_fn = get_input_fn(TEST_INPUT, BATCH_SIZE)\n",
    "my_test_input_fn = get_my_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lE4c3ELMQjHJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Testing the input function\\nwith tf.Graph().as_default():\\n    train_input = train_input_fn()\\n    with tf.train.MonitoredSession() as sess:\\n        print (train_input)\\n        print (sess.run(train_input))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Testing the input function\n",
    "with tf.Graph().as_default():\n",
    "    train_input = train_input_fn()\n",
    "    with tf.train.MonitoredSession() as sess:\n",
    "        print (train_input)\n",
    "        print (sess.run(train_input))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VxXAUrYN7TvR"
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        color_name = features[COLOR_NAME_KEY]\n",
    "        sequence_length = features[SEQUENCE_LENGTH_KEY]\n",
    "\n",
    "        # Creating dense representation for the names\n",
    "        # and then converting it to one hot representation\n",
    "        dense_color_name = tf.sparse_tensor_to_dense(color_name, default_value=len(CHARACTERS))\n",
    "        color_name_onehot = tf.one_hot(dense_color_name, depth=len(CHARACTERS) + 1)\n",
    "        \n",
    "        \n",
    "        # Each RNN layer will consist of a LSTM cell\n",
    "        rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=color_name_onehot,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "\n",
    "        loss = None\n",
    "        train_op = None\n",
    "\n",
    "        if mode != tf.contrib.learn.ModeKeys.INFER:    \n",
    "            loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return tf.contrib.learn.ModelFnOps(mode,\n",
    "                                           predictions=predictions,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op)\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gUHR3Mzc7Tvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': None, '_environment': 'local', '_save_summary_steps': 100, '_keep_checkpoint_max': 5, '_num_ps_replicas': 0, '_task_type': None, '_evaluation_master': '', '_is_chief': True, '_task_id': 0, '_master': '', '_save_checkpoints_steps': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7ff0fa2c7710>, '_tf_random_seed': None, '_keep_checkpoint_every_n_hours': 10000, '_save_checkpoints_secs': 600, '_num_worker_replicas': 0, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      "}\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpsvtqmp69\n"
     ]
    }
   ],
   "source": [
    "model_fn = get_model_fn(rnn_cell_sizes=[256, 128], # size of the hidden layers\n",
    "                        label_dimension=3, # since is RGB\n",
    "                        dnn_layer_sizes=[64], # size of units in the dense layers on top of the RNN\n",
    "                        optimizer='Adam',\n",
    "                        learning_rate=0.01)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DUZEKQrdGgZE"
   },
   "outputs": [],
   "source": [
    "# create experiment\n",
    "def generate_experiment_fn():\n",
    "  \n",
    "  \"\"\"\n",
    "  Create an experiment function given hyperparameters.\n",
    "  Returns:\n",
    "    A function (output_dir) -> Experiment where output_dir is a string\n",
    "    representing the location of summaries, checkpoints, and exports.\n",
    "    this function is used by learn_runner to create an Experiment which\n",
    "    executes model code provided in the form of an Estimator and\n",
    "    input functions.\n",
    "    All listed arguments in the outer function are used to create an\n",
    "    Estimator, and input functions (training, evaluation, serving).\n",
    "    Unlisted args are passed through to Experiment.\n",
    "  \"\"\"\n",
    "\n",
    "  def _experiment_fn(output_dir):\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=test_input_fn,\n",
    "        train_steps=STEPS\n",
    "    )\n",
    "  return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpsvtqmp69/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.409742, step = 1\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-07-20:29:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsvtqmp69/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-07-20:29:24\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 0.264347\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 1): loss = 0.264347, global_step = 1\n",
      "INFO:tensorflow:global_step/sec: 7.72342\n",
      "INFO:tensorflow:loss = 0.045325, step = 101 (12.949 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6693\n",
      "INFO:tensorflow:loss = 0.0671752, step = 201 (9.373 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2149\n",
      "INFO:tensorflow:loss = 0.0503306, step = 301 (9.790 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1403\n",
      "INFO:tensorflow:loss = 0.0560174, step = 401 (9.862 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.2552\n",
      "INFO:tensorflow:loss = 0.0425734, step = 501 (8.885 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9685\n",
      "INFO:tensorflow:loss = 0.0489241, step = 601 (9.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9651\n",
      "INFO:tensorflow:loss = 0.0417212, step = 701 (9.120 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.87301\n",
      "INFO:tensorflow:loss = 0.043939, step = 801 (10.129 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.94989\n",
      "INFO:tensorflow:loss = 0.0407266, step = 901 (10.050 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0203\n",
      "INFO:tensorflow:loss = 0.0402362, step = 1001 (9.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3113\n",
      "INFO:tensorflow:loss = 0.0468997, step = 1101 (9.699 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6022\n",
      "INFO:tensorflow:loss = 0.043691, step = 1201 (9.431 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3495\n",
      "INFO:tensorflow:loss = 0.0386685, step = 1301 (9.662 sec)\n",
      "INFO:tensorflow:global_step/sec: 11.0226\n",
      "INFO:tensorflow:loss = 0.026513, step = 1401 (9.072 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9159\n",
      "INFO:tensorflow:loss = 0.0336587, step = 1501 (9.161 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5373\n",
      "INFO:tensorflow:loss = 0.0375538, step = 1601 (9.490 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.92699\n",
      "INFO:tensorflow:loss = 0.0237464, step = 1701 (10.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.117\n",
      "INFO:tensorflow:loss = 0.0274299, step = 1801 (9.884 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4335\n",
      "INFO:tensorflow:loss = 0.0302584, step = 1901 (9.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.74767\n",
      "INFO:tensorflow:loss = 0.0201734, step = 2001 (10.259 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.99565\n",
      "INFO:tensorflow:loss = 0.0208108, step = 2101 (10.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4117\n",
      "INFO:tensorflow:loss = 0.0204871, step = 2201 (9.605 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.492\n",
      "INFO:tensorflow:loss = 0.0217164, step = 2301 (9.531 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.704\n",
      "INFO:tensorflow:loss = 0.0190827, step = 2401 (9.342 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95261\n",
      "INFO:tensorflow:loss = 0.0131511, step = 2501 (10.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.64648\n",
      "INFO:tensorflow:loss = 0.016781, step = 2601 (10.366 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8153\n",
      "INFO:tensorflow:loss = 0.0166779, step = 2701 (9.246 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2438\n",
      "INFO:tensorflow:loss = 0.0194124, step = 2801 (9.762 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3318\n",
      "INFO:tensorflow:loss = 0.0123026, step = 2901 (9.679 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3716\n",
      "INFO:tensorflow:loss = 0.0130253, step = 3001 (9.642 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7408\n",
      "INFO:tensorflow:loss = 0.0178805, step = 3101 (9.311 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5556\n",
      "INFO:tensorflow:loss = 0.0155669, step = 3201 (9.473 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.9524\n",
      "INFO:tensorflow:loss = 0.019155, step = 3301 (9.130 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4302\n",
      "INFO:tensorflow:loss = 0.00980961, step = 3401 (9.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.66812\n",
      "INFO:tensorflow:loss = 0.0146369, step = 3501 (10.344 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0418\n",
      "INFO:tensorflow:loss = 0.0106311, step = 3601 (9.958 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.136\n",
      "INFO:tensorflow:loss = 0.0131354, step = 3701 (9.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.3149\n",
      "INFO:tensorflow:loss = 0.00836282, step = 3801 (9.695 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5156\n",
      "INFO:tensorflow:loss = 0.0128562, step = 3901 (9.510 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.898\n",
      "INFO:tensorflow:loss = 0.0157794, step = 4001 (9.176 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2967\n",
      "INFO:tensorflow:loss = 0.010889, step = 4101 (9.712 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.022\n",
      "INFO:tensorflow:loss = 0.010753, step = 4201 (9.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.28051\n",
      "INFO:tensorflow:loss = 0.00916871, step = 4301 (10.775 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2745\n",
      "INFO:tensorflow:loss = 0.0101932, step = 4401 (9.733 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.96907\n",
      "INFO:tensorflow:loss = 0.00930309, step = 4501 (10.031 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2873\n",
      "INFO:tensorflow:loss = 0.010148, step = 4601 (9.721 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1251\n",
      "INFO:tensorflow:loss = 0.00833915, step = 4701 (9.877 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7525\n",
      "INFO:tensorflow:loss = 0.00857749, step = 4801 (9.300 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.0012\n",
      "INFO:tensorflow:loss = 0.0101708, step = 4901 (9.999 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.8665\n",
      "INFO:tensorflow:loss = 0.00714394, step = 5001 (9.202 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.81113\n",
      "INFO:tensorflow:loss = 0.0113473, step = 5101 (10.193 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.95148\n",
      "INFO:tensorflow:loss = 0.00631246, step = 5201 (10.049 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4294\n",
      "INFO:tensorflow:loss = 0.00512506, step = 5301 (9.588 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1448\n",
      "INFO:tensorflow:loss = 0.010148, step = 5401 (9.857 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4345\n",
      "INFO:tensorflow:loss = 0.0072174, step = 5501 (9.584 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5288\n",
      "INFO:tensorflow:loss = 0.0108916, step = 5601 (9.498 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.7922\n",
      "INFO:tensorflow:loss = 0.00546897, step = 5701 (9.266 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4762\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:loss = 0.0101217, step = 5801 (9.546 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.41204\n",
      "INFO:tensorflow:loss = 0.00885783, step = 5901 (10.625 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.83761\n",
      "INFO:tensorflow:loss = 0.00681117, step = 6001 (11.315 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.266\n",
      "INFO:tensorflow:loss = 0.0099501, step = 6101 (9.741 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 6143 into /tmp/tmpsvtqmp69/model.ckpt.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-07-20:39:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsvtqmp69/model.ckpt-6143\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-07-20:39:24\n",
      "INFO:tensorflow:Saving dict for global step 6143: global_step = 6143, loss = 0.0465886\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n",
      "INFO:tensorflow:Validation (step 6143): loss = 0.0465886, global_step = 6143\n",
      "INFO:tensorflow:global_step/sec: 7.69177\n",
      "INFO:tensorflow:loss = 0.00931769, step = 6201 (13.001 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.73026\n",
      "INFO:tensorflow:loss = 0.00512375, step = 6301 (10.277 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2795\n",
      "INFO:tensorflow:loss = 0.00885814, step = 6401 (9.728 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.1157\n",
      "INFO:tensorflow:loss = 0.00572086, step = 6501 (9.886 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.6247\n",
      "INFO:tensorflow:loss = 0.00827183, step = 6601 (9.412 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.708\n",
      "INFO:tensorflow:loss = 0.00625515, step = 6701 (9.339 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.62158\n",
      "INFO:tensorflow:loss = 0.00928472, step = 6801 (10.393 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.91255\n",
      "INFO:tensorflow:loss = 0.00657618, step = 6901 (10.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.4018\n",
      "INFO:tensorflow:loss = 0.0127396, step = 7001 (9.614 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5064\n",
      "INFO:tensorflow:loss = 0.00661422, step = 7101 (9.518 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2248\n",
      "INFO:tensorflow:loss = 0.00900109, step = 7201 (9.780 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.5456\n",
      "INFO:tensorflow:loss = 0.0084168, step = 7301 (9.483 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.2115\n",
      "INFO:tensorflow:loss = 0.00538284, step = 7401 (9.793 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.17331\n",
      "INFO:tensorflow:loss = 0.00669943, step = 7501 (10.901 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.82396\n",
      "INFO:tensorflow:loss = 0.00520482, step = 7601 (17.171 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.41031\n",
      "INFO:tensorflow:loss = 0.00421056, step = 7701 (18.482 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.60198\n",
      "INFO:tensorflow:loss = 0.00742673, step = 7801 (17.851 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.66523\n",
      "INFO:tensorflow:loss = 0.00781029, step = 7901 (17.652 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.0177\n",
      "INFO:tensorflow:loss = 0.0105221, step = 8001 (16.618 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.93776\n",
      "INFO:tensorflow:loss = 0.00635853, step = 8101 (16.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.14921\n",
      "INFO:tensorflow:loss = 0.0069383, step = 8201 (16.263 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.82654\n",
      "INFO:tensorflow:loss = 0.00615147, step = 8301 (11.327 sec)\n",
      "INFO:tensorflow:global_step/sec: 10.862\n",
      "INFO:tensorflow:loss = 0.00735537, step = 8401 (9.206 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.60931\n",
      "INFO:tensorflow:loss = 0.0079703, step = 8501 (10.407 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.50089\n",
      "INFO:tensorflow:loss = 0.00588366, step = 8601 (10.525 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.55495\n",
      "INFO:tensorflow:loss = 0.00694388, step = 8701 (11.690 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.26709\n",
      "INFO:tensorflow:loss = 0.00585587, step = 8801 (15.956 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94005\n",
      "INFO:tensorflow:loss = 0.00471021, step = 8901 (16.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.17942\n",
      "INFO:tensorflow:loss = 0.00363866, step = 9001 (16.181 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.1085\n",
      "INFO:tensorflow:loss = 0.00537318, step = 9101 (16.370 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.16746\n",
      "INFO:tensorflow:loss = 0.00534841, step = 9201 (16.214 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.84812\n",
      "INFO:tensorflow:loss = 0.0110652, step = 9301 (17.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.67769\n",
      "INFO:tensorflow:loss = 0.00537939, step = 9401 (17.613 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.97496\n",
      "INFO:tensorflow:loss = 0.00549104, step = 9501 (16.736 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.53013\n",
      "INFO:tensorflow:loss = 0.00350414, step = 9601 (18.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.94437\n",
      "INFO:tensorflow:loss = 0.0067591, step = 9701 (16.824 sec)\n",
      "INFO:tensorflow:global_step/sec: 5.98749\n",
      "INFO:tensorflow:loss = 0.00447664, step = 9801 (16.700 sec)\n",
      "INFO:tensorflow:global_step/sec: 6.27708\n",
      "INFO:tensorflow:loss = 0.00409213, step = 9901 (15.931 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmpsvtqmp69/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.00319448.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-07-20:48:10\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsvtqmp69/model.ckpt-10000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-07-20:48:13\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 0.0443338\n",
      "WARNING:tensorflow:Skipping summary for global_step, must be a float or np.float32.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 10000, 'loss': 0.044333812}, [])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run experiment \n",
    "learn_runner.run(generate_experiment_fn(), '/tmp/outputdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1496848787689,
     "user": {
      "displayName": "Jamie Smith",
      "photoUrl": "//lh3.googleusercontent.com/-lH1_9xAmlJs/AAAAAAAAAAI/AAAAAAAAAGc/MXmYSfunJKo/s50-c-k-no/photo.jpg",
      "userId": "116166808337702908693"
     },
     "user_tz": 240
    },
    "id": "8o5w8lXiGk_1",
    "outputId": "cfb93135-f21c-48bc-d6ca-c28cfc266ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsvtqmp69/model.ckpt-10000\n",
      "[  74.92446136  176.39160156   66.25933838]\n",
      "[ 146.85467529   17.46886635   33.10726547]\n",
      "[ 232.80462646  230.7137146   224.69525146]\n",
      "[ 178.29762268   64.12382507   64.99927521]\n",
      "[ 98.00068665  78.83456421  69.97039032]\n"
     ]
    }
   ],
   "source": [
    "p2 = estimator.predict(input_fn=test_input_fn, as_iterable=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(next(p2) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpsvtqmp69/model.ckpt-10000\n",
      "\n",
      "orange rgb (253, 110, 34)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0VJwEC2F/wOzSaRr+qYb7gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0e8699748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow orange rgb (240, 143, 69)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWgoOoqXgIFoKDqKl8D9na6SPQhys6AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0e8744e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diamond rgb (198, 241, 240)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWgoOoqXgIFoKDqKl8D/o+anbmL97kwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0f8e8f908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple blue rgb (102, 51, 194)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEC0FB9FScBAtBQfRUvgffzWkSdTq2egAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0e87a58d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple red rgb (168, 20, 75)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npeAgWgoOoqXgIFoKDqKl8D8euaQRpffvJwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0c47450b8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple rgb (140, 52, 164)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKTiIloKDaCk4iJaCg2gp/A/R4KRQwKRFugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0f87215c0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water rgb (183, 214, 227)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noqXgIFoKDqKl4CBaCv8DhgKpuwBFWCQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff08e185a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fire rgb (165, 71, 14)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFBxES8FBtBT+Dx5npAUbdAZ/AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0f84e8cc0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jamie rgb (215, 188, 161)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0VJwEC0FB9FScBAthf8BKpSpmiZo+pMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7ff0c475ea90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = estimator.predict(input_fn=my_test_input_fn, as_iterable=True)\n",
    "\n",
    "color_names = _get_csv_column(MY_TEST_INPUT, 'name')\n",
    "\n",
    "print()\n",
    "for p, name in zip(preds, color_names):\n",
    "    color = tuple(map(int, p * 255))\n",
    "    print(name, 'rgb', color)\n",
    "    _plot_rgb(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//experimental/users/jamieas/transform_colab:notebook",
    "kind": "private"
   },
   "name": "Copy of CustomEstimator.ipynb",
   "provenance": [
    {
     "file_id": "0BwN-JPfIIHwgdFkwUTVIWTQwU00",
     "timestamp": 1496845355496
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
