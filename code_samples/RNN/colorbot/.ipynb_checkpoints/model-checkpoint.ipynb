{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "mDT8S9C9CYtr"
   },
   "outputs": [],
   "source": [
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Feeding function for enqueue data\n",
    "# \n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions as ff\n",
    "\n",
    "# Rnn common functions\n",
    "from tensorflow.contrib.learn.python.learn.estimators import rnn_common\n",
    "\n",
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Input function\n",
    "from tensorflow.python.estimator.inputs import numpy_io\n",
    "\n",
    "# Helpers for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Plot images with pyplot\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UrAyWt23AtCM"
   },
   "outputs": [],
   "source": [
    "# Data files\n",
    "TRAIN_INPUT = 'data/train.csv'\n",
    "TEST_INPUT = 'data/test.csv'\n",
    "MY_TEST_INPUT = 'data/mytest.csv'\n",
    "\n",
    "# Parameters for training\n",
    "STEPS = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for data processing\n",
    "CHARACTERS = [chr(i) for i in range(256)]\n",
    "\n",
    "SEQUENCE_LENGTH_KEY = 'sequence_length'\n",
    "COLOR_NAME_KEY = 'color_name'\n",
    "RGB_KEY = 'rgb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0dlZ9C27M-bS"
   },
   "outputs": [],
   "source": [
    "# This function creates a sparse tensor in the following way, given:\n",
    "# indices = [[0, 0], [1, 1], [2, 2]]\n",
    "# values = [1, 2, 3]\n",
    "# dense_shape = [3, 4]\n",
    "#\n",
    "# The output will be a sparse tensor that represents this dense tensor:\n",
    "# [ \n",
    "#   [1, 0, 0, 0]\n",
    "#   [0, 2, 0, 0]\n",
    "#   [0, 0, 3, 0]\n",
    "# ]\n",
    "#\n",
    "# We're using this to generate a Sparse tensor that can be easily\n",
    "# formated in a one hot representation.\n",
    "# More at: https://www.tensorflow.org/api_docs/python/tf/SparseTensor\n",
    "def _sparse_string_to_index(sp, mapping):\n",
    "    return tf.SparseTensor(indices=sp.indices,\n",
    "                           values=tf.contrib.lookup.string_to_index(sp.values,\n",
    "                                                                    mapping),\n",
    "                           dense_shape=sp.dense_shape)\n",
    "\n",
    "# Returns the column values from a CSV file as a list\n",
    "def _get_csv_column(csv_file, column_name):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        return df[column_name].tolist()\n",
    "\n",
    "# Plot a color image\n",
    "def _plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input function used for training and testing                                                \n",
    "def get_input_fn(csv_file, batch_size, epochs=1):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        \n",
    "        # Sequence length is used by the Dynamic RNN\n",
    "        # to dynamically unroll the graph :D!\n",
    "        df['sequence_length'] = df.name.str.len().astype(np.int32)\n",
    "\n",
    "        def input_fn():\n",
    "            # Using queue with multiple threads to make it scalable\n",
    "            pandas_queue = ff._enqueue_data(df,\n",
    "                                            capacity=1024,\n",
    "                                            shuffle=True,\n",
    "                                            min_after_dequeue=256,\n",
    "                                            num_threads=4,\n",
    "                                            enqueue_size=16,\n",
    "                                            num_epochs=epochs)\n",
    "\n",
    "            _, color_name, r, g, b, seq_len = pandas_queue.dequeue_up_to(batch_size)\n",
    "\n",
    "            # Split strings into chars\n",
    "            split_color_name = tf.string_split(color_name, delimiter='')\n",
    "            # Creating a tf constant to hold the map char -> index\n",
    "            # this is need to create the sparse tensor and after the one hot encode\n",
    "            mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "            # Names represented in a sparse tensor\n",
    "            integerized_color_name = _sparse_string_to_index(split_color_name, mapping)\n",
    "\n",
    "            # Tensor of normalized RGB values\n",
    "            rgb = tf.to_float(tf.stack([r, g, b], axis=1)) / 255.0\n",
    "\n",
    "            # Generates batcheds\n",
    "            batched = tf.train.shuffle_batch({COLOR_NAME_KEY: integerized_color_name,\n",
    "                                              SEQUENCE_LENGTH_KEY: seq_len,\n",
    "                                              RGB_KEY: rgb},\n",
    "                                             batch_size,\n",
    "                                             min_after_dequeue=100,\n",
    "                                             num_threads=4,\n",
    "                                             capacity=1000,\n",
    "                                             enqueue_many=True,\n",
    "                                             allow_smaller_final_batch=True)\n",
    "            label = batched.pop(RGB_KEY)\n",
    "            return batched, label\n",
    "    return input_fn\n",
    "\n",
    "# Creating my own input function for a custom CSV file\n",
    "# it's simpler than the input_fn above but just used for small tests\n",
    "def get_my_input_fn():\n",
    "    def _input_fn():\n",
    "        with open(MY_TEST_INPUT, 'r') as f:\n",
    "            df = pd.read_csv(f)\n",
    "            df['sequence_length'] = df.name.str.len().astype(np.int32)\n",
    "\n",
    "            color_name = df.name.tolist()\n",
    "    \n",
    "            split_color_name = tf.string_split(color_name, delimiter='')\n",
    "            mapping = tf.constant(CHARACTERS, name=\"mapping\")\n",
    "            integerized_color_name = _sparse_string_to_index(split_color_name, mapping)\n",
    "\n",
    "            x = {COLOR_NAME_KEY: integerized_color_name, SEQUENCE_LENGTH_KEY: df.sequence_length.tolist()}\n",
    "\n",
    "            y = np.asarray([[0, 0, 0]], dtype=np.float32)\n",
    "\n",
    "            return x, y\n",
    "    return _input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m5UJyvW5P0Sy"
   },
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE, None)\n",
    "test_input_fn = get_input_fn(TEST_INPUT, BATCH_SIZE)\n",
    "my_test_input_fn = get_my_input_fn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "lE4c3ELMQjHJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Testing the input function\\nwith tf.Graph().as_default():\\n    train_input = train_input_fn()\\n    with tf.train.MonitoredSession() as sess:\\n        print (train_input)\\n        print (sess.run(train_input))\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# Testing the input function\n",
    "with tf.Graph().as_default():\n",
    "    train_input = train_input_fn()\n",
    "    with tf.train.MonitoredSession() as sess:\n",
    "        print (train_input)\n",
    "        print (sess.run(train_input))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Estimator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VxXAUrYN7TvR"
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        color_name = features[COLOR_NAME_KEY]\n",
    "        sequence_length = features[SEQUENCE_LENGTH_KEY]\n",
    "\n",
    "        # Creating dense representation for the names\n",
    "        # and then converting it to one hot representation\n",
    "        dense_color_name = tf.sparse_tensor_to_dense(color_name, default_value=len(CHARACTERS))\n",
    "        color_name_onehot = tf.one_hot(dense_color_name, depth=len(CHARACTERS) + 1)\n",
    "        \n",
    "        \n",
    "        # Each RNN layer will consist of a LSTM cell\n",
    "        rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=color_name_onehot,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "\n",
    "        loss = None\n",
    "        train_op = None\n",
    "\n",
    "        if mode != tf.contrib.learn.ModeKeys.INFER:    \n",
    "            loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return tf.contrib.learn.ModelFnOps(mode,\n",
    "                                           predictions=predictions,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op)\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gUHR3Mzc7Tvb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpe4nd9z8n\n",
      "INFO:tensorflow:Using config: {'_task_type': None, '_master': '', '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9c1d4e8278>, '_is_chief': True, '_tf_random_seed': None, '_session_config': None, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1\n",
      "}\n",
      ", '_task_id': 0, '_num_worker_replicas': 0, '_save_summary_steps': 100, '_model_dir': '/tmp/tmpe4nd9z8n', '_environment': 'local', '_evaluation_master': '', '_keep_checkpoint_max': 5, '_save_checkpoints_steps': None, '_keep_checkpoint_every_n_hours': 10000, '_num_ps_replicas': 0, '_save_checkpoints_secs': 600}\n"
     ]
    }
   ],
   "source": [
    "model_fn = get_model_fn(rnn_cell_sizes=[256, 128], # size of the hidden layers\n",
    "                        label_dimension=3, # since is RGB\n",
    "                        dnn_layer_sizes=[128], # size of units in the dense layers on top of the RNN\n",
    "                        optimizer='Adam',\n",
    "                        learning_rate=0.01)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and Run Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "DUZEKQrdGgZE"
   },
   "outputs": [],
   "source": [
    "# create experiment\n",
    "def generate_experiment_fn():\n",
    "  \n",
    "  \"\"\"\n",
    "  Create an experiment function given hyperparameters.\n",
    "  Returns:\n",
    "    A function (output_dir) -> Experiment where output_dir is a string\n",
    "    representing the location of summaries, checkpoints, and exports.\n",
    "    this function is used by learn_runner to create an Experiment which\n",
    "    executes model code provided in the form of an Estimator and\n",
    "    input functions.\n",
    "    All listed arguments in the outer function are used to create an\n",
    "    Estimator, and input functions (training, evaluation, serving).\n",
    "    Unlisted args are passed through to Experiment.\n",
    "  \"\"\"\n",
    "\n",
    "  def _experiment_fn(output_dir):\n",
    "    return tf.contrib.learn.Experiment(\n",
    "        estimator,\n",
    "        train_input_fn=train_input_fn,\n",
    "        eval_input_fn=test_input_fn,\n",
    "        train_steps=STEPS\n",
    "    )\n",
    "  return _experiment_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.4/dist-packages/tensorflow/contrib/learn/python/learn/monitors.py:268: BaseMonitor.__init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/tensorflow/python/ops/gradients_impl.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/tmpe4nd9z8n/model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "INFO:tensorflow:loss = 0.380736, step = 1\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-16-13:28:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe4nd9z8n/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-16-13:28:57\n",
      "INFO:tensorflow:Saving dict for global step 1: global_step = 1, loss = 0.153228\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "INFO:tensorflow:Validation (step 1): loss = 0.153228, global_step = 1\n",
      "INFO:tensorflow:global_step/sec: 8.03872\n",
      "INFO:tensorflow:loss = 0.0681183, step = 101 (12.441 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.15964\n",
      "INFO:tensorflow:loss = 0.0506721, step = 201 (10.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.45452\n",
      "INFO:tensorflow:loss = 0.0361946, step = 301 (10.577 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.00996\n",
      "INFO:tensorflow:loss = 0.0254957, step = 401 (11.099 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.29535\n",
      "INFO:tensorflow:loss = 0.0115984, step = 501 (10.758 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.99201\n",
      "INFO:tensorflow:loss = 0.0070594, step = 601 (11.121 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.2774\n",
      "INFO:tensorflow:loss = 0.00307138, step = 701 (10.779 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.25852\n",
      "INFO:tensorflow:loss = 0.00298521, step = 801 (10.801 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.10227\n",
      "INFO:tensorflow:loss = 0.00105346, step = 901 (10.986 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.41886\n",
      "INFO:tensorflow:loss = 0.000656435, step = 1001 (11.878 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.93724\n",
      "INFO:tensorflow:loss = 0.000731416, step = 1101 (11.189 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.33497\n",
      "INFO:tensorflow:loss = 0.000905898, step = 1201 (11.998 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12785\n",
      "INFO:tensorflow:loss = 0.000514407, step = 1301 (10.955 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.11552\n",
      "INFO:tensorflow:loss = 0.000737661, step = 1401 (10.970 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.8255\n",
      "INFO:tensorflow:loss = 0.00126053, step = 1501 (11.331 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.33454\n",
      "INFO:tensorflow:loss = 0.00145432, step = 1601 (10.713 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12408\n",
      "INFO:tensorflow:loss = 0.000856629, step = 1701 (10.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.73212\n",
      "INFO:tensorflow:loss = 0.00072851, step = 1801 (11.452 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.74221\n",
      "INFO:tensorflow:loss = 0.000531267, step = 1901 (11.439 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.37846\n",
      "INFO:tensorflow:loss = 0.000374185, step = 2001 (11.935 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.90823\n",
      "INFO:tensorflow:loss = 0.000345876, step = 2101 (11.226 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.56437\n",
      "INFO:tensorflow:loss = 0.000493265, step = 2201 (11.676 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.03338\n",
      "INFO:tensorflow:loss = 0.000421313, step = 2301 (11.070 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.91932\n",
      "INFO:tensorflow:loss = 0.000703132, step = 2401 (11.212 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.08797\n",
      "INFO:tensorflow:loss = 0.0010586, step = 2501 (11.004 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.0156\n",
      "INFO:tensorflow:loss = 0.00156196, step = 2601 (11.092 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.92023\n",
      "INFO:tensorflow:loss = 0.0015737, step = 2701 (11.210 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.90109\n",
      "INFO:tensorflow:loss = 0.00109582, step = 2801 (11.235 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.10338\n",
      "INFO:tensorflow:loss = 0.00102142, step = 2901 (10.985 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.94506\n",
      "INFO:tensorflow:loss = 0.000333857, step = 3001 (11.179 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.20299\n",
      "INFO:tensorflow:loss = 0.000180187, step = 3101 (10.866 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.03048\n",
      "INFO:tensorflow:loss = 0.000157876, step = 3201 (11.074 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.79027\n",
      "INFO:tensorflow:loss = 0.00019827, step = 3301 (11.376 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.07113\n",
      "INFO:tensorflow:loss = 0.000178429, step = 3401 (11.024 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12095\n",
      "INFO:tensorflow:loss = 0.000374867, step = 3501 (10.964 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.24153\n",
      "INFO:tensorflow:loss = 0.000563293, step = 3601 (10.821 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.1594\n",
      "INFO:tensorflow:loss = 0.00115015, step = 3701 (10.918 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.17549\n",
      "INFO:tensorflow:loss = 0.0010854, step = 3801 (10.899 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.61948\n",
      "INFO:tensorflow:loss = 0.00649331, step = 3901 (11.602 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.89498\n",
      "INFO:tensorflow:loss = 0.00750369, step = 4001 (11.242 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.28468\n",
      "INFO:tensorflow:loss = 0.00105422, step = 4101 (10.770 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.0348\n",
      "INFO:tensorflow:loss = 0.000150031, step = 4201 (11.068 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.21293\n",
      "INFO:tensorflow:loss = 2.53878e-05, step = 4301 (10.854 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.34065\n",
      "INFO:tensorflow:loss = 1.0859e-05, step = 4401 (10.706 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.11629\n",
      "INFO:tensorflow:loss = 6.5534e-06, step = 4501 (10.969 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.94714\n",
      "INFO:tensorflow:loss = 4.14681e-06, step = 4601 (11.177 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.06239\n",
      "INFO:tensorflow:loss = 3.67107e-06, step = 4701 (11.035 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.05797\n",
      "INFO:tensorflow:loss = 3.29135e-06, step = 4801 (11.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.07605\n",
      "INFO:tensorflow:loss = 7.27637e-06, step = 4901 (11.018 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.38887\n",
      "INFO:tensorflow:loss = 1.31248e-05, step = 5001 (10.651 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.19098\n",
      "INFO:tensorflow:loss = 4.46576e-05, step = 5101 (10.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.13583\n",
      "INFO:tensorflow:loss = 0.000232603, step = 5201 (10.946 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.14845\n",
      "INFO:tensorflow:loss = 0.000556639, step = 5301 (10.931 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 5398 into /tmp/tmpe4nd9z8n/model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-16-13:38:56\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe4nd9z8n/model.ckpt-5398\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-16-13:38:57\n",
      "INFO:tensorflow:Saving dict for global step 5398: global_step = 5398, loss = 0.0656966\n",
      "INFO:tensorflow:Validation (step 5398): loss = 0.0656966, global_step = 5398\n",
      "INFO:tensorflow:global_step/sec: 7.91289\n",
      "INFO:tensorflow:loss = 0.000562866, step = 5401 (12.638 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.06849\n",
      "INFO:tensorflow:loss = 0.000632162, step = 5501 (11.027 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.08919\n",
      "INFO:tensorflow:loss = 0.0004538, step = 5601 (11.002 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.96361\n",
      "INFO:tensorflow:loss = 0.000371844, step = 5701 (11.156 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.19092\n",
      "INFO:tensorflow:loss = 0.000347585, step = 5801 (10.880 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.16922\n",
      "INFO:tensorflow:loss = 0.000239176, step = 5901 (10.906 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.14144\n",
      "INFO:tensorflow:loss = 0.000283171, step = 6001 (10.939 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.16326\n",
      "INFO:tensorflow:loss = 0.000331486, step = 6101 (10.913 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.27805\n",
      "INFO:tensorflow:loss = 0.000696147, step = 6201 (10.778 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.23632\n",
      "INFO:tensorflow:loss = 0.00070081, step = 6301 (10.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.18473\n",
      "INFO:tensorflow:loss = 0.000724807, step = 6401 (10.888 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.283\n",
      "INFO:tensorflow:loss = 0.000506942, step = 6501 (10.772 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.20555\n",
      "INFO:tensorflow:loss = 0.000446755, step = 6601 (10.863 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.37277\n",
      "INFO:tensorflow:loss = 0.000349835, step = 6701 (10.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.23633\n",
      "INFO:tensorflow:loss = 0.000370567, step = 6801 (10.827 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.16659\n",
      "INFO:tensorflow:loss = 0.000243953, step = 6901 (10.909 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.22814\n",
      "INFO:tensorflow:loss = 0.000323547, step = 7001 (10.836 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.20871\n",
      "INFO:tensorflow:loss = 0.000291171, step = 7101 (10.859 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.26986\n",
      "INFO:tensorflow:loss = 0.000478483, step = 7201 (10.788 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.17005\n",
      "INFO:tensorflow:loss = 0.000630453, step = 7301 (10.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.37299\n",
      "INFO:tensorflow:loss = 0.000869131, step = 7401 (10.669 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.10871\n",
      "INFO:tensorflow:loss = 0.00123528, step = 7501 (10.978 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.05432\n",
      "INFO:tensorflow:loss = 0.00127932, step = 7601 (11.044 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.06012\n",
      "INFO:tensorflow:loss = 0.00102794, step = 7701 (11.037 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.17064\n",
      "INFO:tensorflow:loss = 0.000269781, step = 7801 (10.904 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.10399\n",
      "INFO:tensorflow:loss = 0.00011759, step = 7901 (10.984 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.12367\n",
      "INFO:tensorflow:loss = 3.90509e-05, step = 8001 (10.960 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.22361\n",
      "INFO:tensorflow:loss = 3.66963e-05, step = 8101 (10.842 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.01757\n",
      "INFO:tensorflow:loss = 3.09657e-05, step = 8201 (11.089 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.05837\n",
      "INFO:tensorflow:loss = 4.59105e-05, step = 8301 (11.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.252\n",
      "INFO:tensorflow:loss = 8.93181e-05, step = 8401 (10.808 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.67726\n",
      "INFO:tensorflow:loss = 9.02942e-05, step = 8501 (11.524 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.11332\n",
      "INFO:tensorflow:loss = 0.000250271, step = 8601 (10.973 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.92104\n",
      "INFO:tensorflow:loss = 0.000543159, step = 8701 (11.209 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.18544\n",
      "INFO:tensorflow:loss = 0.001526, step = 8801 (10.887 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.05139\n",
      "INFO:tensorflow:loss = 0.00172933, step = 8901 (11.048 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.09773\n",
      "INFO:tensorflow:loss = 0.018382, step = 9001 (10.992 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.11524\n",
      "INFO:tensorflow:loss = 0.00380688, step = 9101 (10.971 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.14332\n",
      "INFO:tensorflow:loss = 0.000381894, step = 9201 (10.937 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.85183\n",
      "INFO:tensorflow:loss = 0.000139564, step = 9301 (11.297 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.16969\n",
      "INFO:tensorflow:loss = 2.94528e-05, step = 9401 (10.905 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.22951\n",
      "INFO:tensorflow:loss = 8.49922e-06, step = 9501 (10.835 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.74284\n",
      "INFO:tensorflow:loss = 8.8555e-06, step = 9601 (11.438 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.16397\n",
      "INFO:tensorflow:loss = 3.98404e-06, step = 9701 (10.912 sec)\n",
      "INFO:tensorflow:global_step/sec: 8.90129\n",
      "INFO:tensorflow:loss = 6.29251e-06, step = 9801 (11.234 sec)\n",
      "INFO:tensorflow:global_step/sec: 9.29147\n",
      "INFO:tensorflow:loss = 1.1497e-05, step = 9901 (10.763 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 10000 into /tmp/tmpe4nd9z8n/model.ckpt.\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n",
      "INFO:tensorflow:Loss for final step: 1.14464e-05.\n",
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Starting evaluation at 2017-06-16-13:47:22\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe4nd9z8n/model.ckpt-10000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-06-16-13:47:23\n",
      "INFO:tensorflow:Saving dict for global step 10000: global_step = 10000, loss = 0.0655044\n",
      "WARNING:tensorflow:Error encountered when serializing LAYER_NAME_UIDS.\n",
      "Type is unsupported, or the types of the items don't match field type in CollectionDef.\n",
      "'dict' object has no attribute 'name'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'global_step': 10000, 'loss': 0.065504394}, [])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run experiment \n",
    "learn_runner.run(generate_experiment_fn(), '/tmp/outputdir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 252,
     "status": "ok",
     "timestamp": 1496848787689,
     "user": {
      "displayName": "Jamie Smith",
      "photoUrl": "//lh3.googleusercontent.com/-lH1_9xAmlJs/AAAAAAAAAAI/AAAAAAAAAGc/MXmYSfunJKo/s50-c-k-no/photo.jpg",
      "userId": "116166808337702908693"
     },
     "user_tz": 240
    },
    "id": "8o5w8lXiGk_1",
    "outputId": "cfb93135-f21c-48bc-d6ca-c28cfc266ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:enqueue_data was called with num_epochs and num_threads > 1. num_epochs is applied per thread, so this will produce more epochs than you probably intend. If you want to limit epochs, use one thread.\n",
      "WARNING:tensorflow:enqueue_data was called with shuffle=True, num_threads > 1, and num_epochs. This will create multiple threads, all reading the array/dataframe in order adding to the same shuffling queue; the results will likely not be sufficiently shuffled.\n",
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe4nd9z8n/model.ckpt-10000\n",
      "[ 161.93147278   60.56196213  168.82467651]\n",
      "[ 246.00631714  184.02256775  185.76617432]\n",
      "[  45.1961937   173.1978302   216.95178223]\n",
      "[  22.03488922  237.71044922  224.44076538]\n",
      "[ 116.02417755    6.25331688  193.49478149]\n"
     ]
    }
   ],
   "source": [
    "p2 = estimator.predict(input_fn=test_input_fn, as_iterable=True)\n",
    "\n",
    "for i in range(5):\n",
    "    print(next(p2) * 255)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-e89ced8fbafb>:19: string_to_index (from tensorflow.contrib.lookup.lookup_ops) is deprecated and will be removed after 2017-01-07.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to index_table_from_tensor and call the lookup method of the returned table.\n",
      "INFO:tensorflow:Restoring parameters from /tmp/tmpe4nd9z8n/model.ckpt-10000\n",
      "\n",
      "orange rgb (255, 104, 25)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwUG0FBxES+H/4XykYiFgW1UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1ee15a20>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow orange rgb (222, 110, 51)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ncBAtBQfRUnAQLYX/A6FKpGCYFKKdAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1d35a668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diamond rgb (185, 241, 253)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ng2gpOIiWgoNoKTiIlsL/AOjrqdt4fD3pAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1d521908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple blue rgb (47, 110, 190)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nWgoOoqXgIFoKDqKl8D9/D6RJYsO+aQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c7d9f5898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple red rgb (187, 3, 55)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CBaCg6ipeAgWgoOoqXgIFoKDqKl8D8GMJ02XrEvVAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c7d1c9eb8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "purple rgb (131, 40, 167)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4CBaCg6ipeAgWgoOoqXwP2YrpD8iZtBbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1d1c4a58>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "water rgb (167, 140, 147)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nB9FScBAtBQfRUnAQLYX/AfbFpJIb8lbRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1eec1b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pink rgb (255, 193, 202)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQbQUHERLwUG0FP4H+VapyRGJDwgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c1eecc470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rock rgb (106, 39, 9)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACPCAYAAAB5wADzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9c7cdfa9e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = estimator.predict(input_fn=my_test_input_fn, as_iterable=True)\n",
    "\n",
    "color_names = _get_csv_column(MY_TEST_INPUT, 'name')\n",
    "\n",
    "print()\n",
    "for p, name in zip(preds, color_names):\n",
    "    color = tuple(map(int, p * 255))\n",
    "    print(name, 'rgb', color)\n",
    "    _plot_rgb(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//experimental/users/jamieas/transform_colab:notebook",
    "kind": "private"
   },
   "name": "Copy of CustomEstimator.ipynb",
   "provenance": [
    {
     "file_id": "0BwN-JPfIIHwgdFkwUTVIWTQwU00",
     "timestamp": 1496845355496
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
