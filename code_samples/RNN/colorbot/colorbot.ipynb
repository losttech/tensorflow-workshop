{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Colorbot\n",
    "\n",
    "**Special thanks to @MarkDaoust that helped us with this material**\n",
    "\n",
    "In order to have a better experience follow these steps:\n",
    "\n",
    "1. Just read all the notebook, try to understand what each part of the code is doing and get familiar with the implementation.\n",
    "2. For each exercise in this notebook make a copy of this notebook and try to implement what is expected. We suggest the following order for the exercises: HYPERPARAMETERS, EXPERIMENT, DATASET\n",
    "3. Troubles or doubts about the code/exercises? Ask the instructor about it or check colorbot_solutions.ipnyb for a possible implementation/instruction if available\n",
    "\n",
    "## Content of this notebook\n",
    "\n",
    "In this notebook you'll find a full implementation of a RNN model using the TensorFlow Estimators including comments and details about how to do it. \n",
    "\n",
    "Once you finish this notebook, you'll have a better understanding of:\n",
    "  * [TensorFlow Estimators](https://www.tensorflow.org/extend/estimators)\n",
    "  * [TensorFlow DataSets](https://github.com/tensorflow/tensorflow/tree/r1.2/tensorflow/contrib/data)\n",
    "  * [RNNs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "## What is colorbot?\n",
    "\n",
    "Colorbot is a RNN model that receives a word (sequence of characters) as input and learns to predict a rgb value that better represents this word. As a result we have a color generator!\n",
    "\n",
    "![colorbot in action](imgs/model_gif.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "mDT8S9C9CYtr"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "# Tensorflow\n",
    "import tensorflow as tf\n",
    "print('Use TensorFlow v1.2 or higher')\n",
    "print('Your TensorFlow version:', tf.__version__) \n",
    "\n",
    "# Feeding function for enqueue data\n",
    "from tensorflow.python.estimator.inputs.queues import feeding_functions as ff\n",
    "\n",
    "# Rnn common functions\n",
    "from tensorflow.contrib.learn.python.learn.estimators import rnn_common\n",
    "\n",
    "# Run an experiment\n",
    "from tensorflow.contrib.learn.python.learn import learn_runner\n",
    "\n",
    "# Model builder\n",
    "from tensorflow.python.estimator import model_fn as model_fn_lib\n",
    "\n",
    "# Plot images with pyplot\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Helpers for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "UrAyWt23AtCM"
   },
   "outputs": [],
   "source": [
    "# Data files\n",
    "TRAIN_INPUT = 'data/train.csv'\n",
    "TEST_INPUT = 'data/test.csv'\n",
    "MY_TEST_INPUT = 'data/mytest.csv'\n",
    "\n",
    "# Parameters for training\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Parameters for data processing\n",
    "VOCAB_SIZE = 256\n",
    "CHARACTERS = [chr(i) for i in range(VOCAB_SIZE)]\n",
    "SEQUENCE_LENGTH_KEY = 'sequence_length'\n",
    "COLOR_NAME_KEY = 'color_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "0dlZ9C27M-bS"
   },
   "outputs": [],
   "source": [
    "# Returns the column values from a CSV file as a list\n",
    "def _get_csv_column(csv_file, column_name):\n",
    "    with open(csv_file, 'r') as f:\n",
    "        df = pd.read_csv(f)\n",
    "        return df[column_name].tolist()\n",
    "\n",
    "# Plots a color image\n",
    "def _plot_rgb(rgb):\n",
    "    data = [[rgb]]\n",
    "    plt.figure(figsize=(2,2))\n",
    "    plt.imshow(data, interpolation='nearest')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input function\n",
    "\n",
    "Here we are defining the input pipeline using the [Dataset API](https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data).\n",
    "\n",
    "One special operation that we're doing is called **group_by_window**, what this function does is to map each consecutive element in this dataset to a key using `key_func` and then groups the elements by key. It then applies `reduce_func` to at most `window_size` elements matching the same key. All except the final window for each key will contain `window_size` elements; the final window may be smaller.\n",
    "\n",
    "In the code below what we're doing is using the group_by_window to batch color names that have similar length together, this makes the code more efficient since the RNN will be unfolded (approximately) the same number of steps in each batch.\n",
    "\n",
    "![](imgs/batch_by_length.png)\n",
    "*Image from [Sequence Models and the RNN API (TensorFlow Dev Summit 2017)](https://www.youtube.com/watch?v=RIR_-Xlbp7s)*\n",
    "\n",
    "** *EXERCISE DATASET (first complete the EXERCISE EXPERIMENT: change the input function bellow so it will just use normal padded_batch instead sorting the batches. Then run each model using experiments and compare the efficiency (time, global_step/sec) using TensorBoard.\n",
    "hint: to compare the implementations using tensorboard just copy the model_dir folder of both executions to the same directory (the model dir should be different at each time you run the model) and point tensorboard to it with: tensorboard --logdir=path_to_model_dirs_par)* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_input_fn(csv_file, batch_size, num_epochs=1, shuffle=True):\n",
    "    def _parse(line):\n",
    "        '''\n",
    "            This function will parse each line of the text,\n",
    "            returning 3 variables.\n",
    "            \n",
    "            Each line contains: name, red, green, blue separated by \",\"\n",
    "            Where:\n",
    "                name: string\n",
    "                red, green, blue: int [0, 255]\n",
    "    \n",
    "            The variables returned are:\n",
    "                color: tensor containing the rgb values normalized that represent the color name.\n",
    "                Each rgb values is an int [0, 1].\n",
    "                \n",
    "                color_name: a sequence of characters. Example: if name is \"blue\"\n",
    "                color_name will be [\"b\", \"l\", \"u\", \"e\"]\n",
    "                \n",
    "                length = len(color_name). Example: if color_name = [\"b\", \"l\", \"u\", \"e\"], then length = 4 \n",
    "        '''\n",
    "    \n",
    "        # split line\n",
    "        items = tf.string_split([line],',').values\n",
    "\n",
    "        # get 3 last values in the line that are the color rgb values\n",
    "        color = tf.string_to_number(items[1:], out_type=tf.float32) / 255.0\n",
    "\n",
    "        # split color_name (first value in the line)\n",
    "        # into a sequence of characters and calculates the length\n",
    "        color_name = tf.string_split([items[0]], '')\n",
    "        length = color_name.indices[-1, 1] + 1 # length = index of last char + 1\n",
    "        color_name = color_name.values\n",
    "        \n",
    "        return color, color_name, length\n",
    "\n",
    "    def _length_bin(length, cast_value=5, max_bin_id=10):\n",
    "        '''\n",
    "        Chooses a bin for a word given it's length.\n",
    "        The goal is to use group_by_window to group words\n",
    "        with the ~ same ~ length in the same bin.\n",
    "\n",
    "        Each bin will have the size of a batch, so it can train faster.\n",
    "        '''\n",
    "        bin_id = tf.cast(length / cast_value, dtype=tf.int64)\n",
    "        return tf.minimum(bin_id, max_bin_id)\n",
    "\n",
    "    def _pad_batch(ds, batch_size):\n",
    "        return ds.padded_batch(batch_size, \n",
    "                               padded_shapes=([None], [None], []),\n",
    "                               padding_values=(0.0, chr(0), tf.cast(0, tf.int64)))\n",
    "\n",
    "    def input_fn():\n",
    "        # https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/data\n",
    "        dataset = (\n",
    "            tf.contrib.data.TextLineDataset(csv_file) # reading from the HD\n",
    "            .skip(1) # skip header\n",
    "            .repeat(num_epochs) # repeat dataset the number of epochs\n",
    "            .map(_parse) # parse each line of text to variables\n",
    "            .group_by_window(key_func=lambda color, color_name, length: _length_bin(length), # choose a bin\n",
    "                             reduce_func=lambda key, ds: _pad_batch(ds, batch_size), # apply reduce funtion\n",
    "                             window_size=batch_size)\n",
    "        )\n",
    "        \n",
    "        # for our \"manual\" test we don't want to shuffle the data\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size=100000)\n",
    "\n",
    "        # create iterator\n",
    "        color, color_name, length = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "        features = {\n",
    "            COLOR_NAME_KEY: color_name,\n",
    "            SEQUENCE_LENGTH_KEY: length,\n",
    "        }\n",
    "\n",
    "        return features, color\n",
    "    return input_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "m5UJyvW5P0Sy"
   },
   "outputs": [],
   "source": [
    "train_input_fn = get_input_fn(TRAIN_INPUT, BATCH_SIZE)\n",
    "test_input_fn = get_input_fn(TEST_INPUT, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Estimator model\n",
    "\n",
    "![](imgs/colorbot_model.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "VxXAUrYN7TvR"
   },
   "outputs": [],
   "source": [
    "def get_model_fn(rnn_cell_sizes,\n",
    "                 label_dimension,\n",
    "                 dnn_layer_sizes=[],\n",
    "                 optimizer='SGD',\n",
    "                 learning_rate=0.01):\n",
    "    \n",
    "    def model_fn(features, labels, mode):\n",
    "        \n",
    "        color_name = features[COLOR_NAME_KEY]\n",
    "        sequence_length = tf.cast(features[SEQUENCE_LENGTH_KEY], dtype=tf.int32) # int64 -> int32\n",
    "        \n",
    "        # ----------- Preparing input --------------------\n",
    "        # Creating a tf constant to hold the map char -> index\n",
    "        # this is need to create the sparse tensor and after the one hot encode\n",
    "        mapping = tf.Variable(CHARACTERS, name=\"mapping\")\n",
    "        table = tf.contrib.lookup.index_table_from_tensor(mapping, dtype=tf.string)\n",
    "        int_color_name = table.lookup(color_name)\n",
    "        \n",
    "        # representing colornames with one hot representation\n",
    "        color_name_onehot = tf.one_hot(int_color_name, depth=len(CHARACTERS) + 1)\n",
    "        \n",
    "        # ---------- RNN -------------------\n",
    "        # Each RNN layer will consist of a LSTM cell\n",
    "        rnn_layers = [tf.contrib.rnn.LSTMCell(size) for size in rnn_cell_sizes]\n",
    "        \n",
    "        # Construct the layers\n",
    "        multi_rnn_cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "        \n",
    "        # Runs the RNN model dynamically\n",
    "        # more about it at: \n",
    "        # https://www.tensorflow.org/api_docs/python/tf/nn/dynamic_rnn\n",
    "        outputs, final_state = tf.nn.dynamic_rnn(cell=multi_rnn_cell,\n",
    "                                                 inputs=color_name_onehot,\n",
    "                                                 sequence_length=sequence_length,\n",
    "                                                 dtype=tf.float32)\n",
    "\n",
    "        # Slice to keep only the last cell of the RNN\n",
    "        last_activations = rnn_common.select_last_activations(outputs,\n",
    "                                                              sequence_length)\n",
    "\n",
    "        # ------------ Dense layers -------------------\n",
    "        # Construct dense layers on top of the last cell of the RNN\n",
    "        for units in dnn_layer_sizes:\n",
    "            last_activations = tf.layers.dense(\n",
    "              last_activations, units, activation=tf.nn.relu)\n",
    "        \n",
    "        # Final dense layer for prediction\n",
    "        predictions = tf.layers.dense(last_activations, label_dimension)\n",
    "\n",
    "        # ----------- Loss and Optimizer ----------------\n",
    "        loss = None\n",
    "        train_op = None\n",
    "\n",
    "        if mode != tf.contrib.learn.ModeKeys.INFER:    \n",
    "            loss = tf.losses.mean_squared_error(labels, predictions)\n",
    "    \n",
    "        if mode == tf.contrib.learn.ModeKeys.TRAIN:    \n",
    "            train_op = tf.contrib.layers.optimize_loss(\n",
    "              loss,\n",
    "              tf.contrib.framework.get_global_step(),\n",
    "              optimizer=optimizer,\n",
    "              learning_rate=learning_rate)\n",
    "        \n",
    "        return model_fn_lib.EstimatorSpec(mode,\n",
    "                                           predictions=predictions,\n",
    "                                           loss=loss,\n",
    "                                           train_op=train_op)\n",
    "    return model_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** *EXERCISE HYPERPARAMETERS: try making changes to the model and see if you can improve the results.\n",
    "Run the original model, run yours and compare them using Tensorboard. What improvements do you see?  \n",
    "hint 0: change the type of RNNCell, maybe a GRUCell? Change the number of hidden layers, or add dnn layers.  \n",
    "hint 1: to compare the implementations using tensorboard just copy the model_dir folder of both executions to the same directory (the model dir should be different at each time you run the model) and point tensorboard to it with: tensorboard --logdir=path_to_model_dirs_par)* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gUHR3Mzc7Tvb"
   },
   "outputs": [],
   "source": [
    "model_fn = get_model_fn(rnn_cell_sizes=[256, 128], # size of the hidden layers\n",
    "                        label_dimension=3, # since is RGB\n",
    "                        dnn_layer_sizes=[128], # size of units in the dense layers on top of the RNN\n",
    "                        optimizer='Adam', # changing optimizer to Adam\n",
    "                        learning_rate=0.01)\n",
    "\n",
    "estimator = tf.estimator.Estimator(model_fn=model_fn, model_dir='colorbot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainning and Evaluating\n",
    "\n",
    "** *EXERCISE EXPERIMENT: The code below works, but we can use an experiment instead. Add a cell that runs an experiment instead of interacting directly with the estimator.  \n",
    "hint 0: you'll need to change the train_input_fn definition, think about it...  \n",
    "hint 1: the change is related with the for loop* **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DUZEKQrdGgZE"
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 40\n",
    "for i in range(NUM_EPOCHS):\n",
    "    print('Training epoch %d' % i)\n",
    "    print('-' * 20)\n",
    "    estimator.train(input_fn=train_input_fn)\n",
    "    print('Evaluating epoch %d' % i)\n",
    "    print('-' * 20)\n",
    "    estimator.evaluate(input_fn = test_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(estimator, input_file):\n",
    "    preds = estimator.predict(input_fn=get_input_fn(input_file, 1, shuffle=False))\n",
    "    color_names = _get_csv_column(input_file, 'name')\n",
    "\n",
    "    print()\n",
    "    for p, name in zip(preds, color_names):\n",
    "        color = tuple(map(int, p * 255))\n",
    "        print(name + ',', 'rgb:', color)\n",
    "        _plot_rgb(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict(estimator, MY_TEST_INPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-trained model predictions\n",
    "\n",
    "In order to load the pre-trained model we can just create an estimator using the model_fn and use the model_dir that contains the pre-trained model files in this case it's 'pretrained'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_estimator = tf.estimator.Estimator(model_dir='pretrained', model_fn=model_fn)\n",
    "predict(pre_estimator, MY_TEST_INPUT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "last_runtime": {
    "build_target": "//experimental/users/jamieas/transform_colab:notebook",
    "kind": "private"
   },
   "name": "Copy of CustomEstimator.ipynb",
   "provenance": [
    {
     "file_id": "0BwN-JPfIIHwgdFkwUTVIWTQwU00",
     "timestamp": 1496845355496
    }
   ],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
